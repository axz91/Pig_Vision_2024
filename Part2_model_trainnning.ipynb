{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7euA2ATzgUG_"
      },
      "source": [
        "### post weaning, predict which day from day 5 to day 7 have a esturs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T-DAebOgqiFL"
      },
      "outputs": [],
      "source": [
        "# Batch 1 total 11, exluded 928 970, 154, 29, missing 44 31 43\n",
        "pig_1  = [\"981\", #\"29\", \"44\", \"31\", \"43\"\n",
        "             \"151\",  \"919\",\n",
        "            \"152\",\"804\",  \"850\", \"974\", \"153\"]\n",
        "\n",
        "estrus_1 = {\n",
        "    '981': ('2022-07-27', \"6\"),\n",
        "    #'29': ('2022-07-29', \"8\"),\n",
        "    '151':('2022-07-26', \"5\"),\n",
        "    #'44': ('2022-07-27', \"6\"),\n",
        "    '919': ('2022-07-26', \"5\"),\n",
        "    #'43': ('2022-07-26', \"5\"),\n",
        "    '152': ('2022-07-26', \"5\"),\n",
        "    '804': ('2022-07-28', \"7\"),\n",
        "    #'31': ('2022-07-26', \"5\"),\n",
        "    '850': ('2022-07-28', \"7\"),\n",
        "    '974': ('2022-07-27', \"6\"),\n",
        "    '153': ('2022-07-27', \"6\"),\n",
        "}\n",
        "\n",
        "# Batch 2 total 9, exluded 931 927 775 850\n",
        "pig_2  =  [ \"138\", \"848\", \"777\", \"809\", \"984\",\n",
        "            \"849\", \"139\", \"866\", \"506\"]\n",
        "estrus_2 = {\n",
        "    '138': ('2022-09-01', \"7\"),\n",
        "    '848': ('2022-08-31', \"6\"),\n",
        "    '777': ('2022-08-31', \"6\"),\n",
        "    '809': ('2022-08-30', \"5\"),\n",
        "    '984': ('2022-08-31', \"6\"),\n",
        "    '849': ('2022-08-31', \"6\"),\n",
        "    '139': ('2022-09-01', \"7\"),\n",
        "    '866': ('2022-08-30', \"5\"),\n",
        "    '506': ('2022-08-30', \"5\"),\n",
        "}\n",
        "\n",
        "# Batch 3 total 19, exluded 16,204,214,97, missed \"73\",\"40\",\"49\", \"15\",\"14\"\n",
        "pig_3 = [\"947\", \"842\", \"950\", \"814\", \"782\", \"158\",\n",
        "         \"951\", \"790\",  \"157\"]\n",
        "estrus_3 = {\n",
        "     '947': ('2022-10-06', \"7\"),\n",
        "    '842': ('2022-10-04', \"5\"),\n",
        "    '950': ('2022-10-06', \"6\"),\n",
        "    '814': ('2022-10-04', \"5\"),\n",
        "    '782': ('2022-10-04', \"5\"),\n",
        "    '158': ('2022-10-05', \"6\"),\n",
        "    '73': ('2022-10-04', \"5\"),\n",
        "    '40': ('2022-10-04', \"5\"),\n",
        "    '49': ('2022-10-06', \"6\"),\n",
        "    '951': ('2022-10-04', \"5\"),\n",
        "    '790': ('2022-10-04', \"5\"),\n",
        "    '15': ('2022-10-04', \"5\"),\n",
        "    '14': ('2022-10-04', \"5\"),\n",
        "    '157': ('2022-10-04', \"5\"),\n",
        "}\n",
        "\n",
        "# Batch 4 total 12, exluded 40987, 40114, 40076, 40032, 44107, 44212, 44219\n",
        "pig_4 = [\n",
        "    \"40054\", \"40086\", \"40051\", \"40040\",\n",
        "    \"40319\", \"40115\", #\"40987\",\n",
        "    \"40141\",\"40883\", \"40026\", \"40867\", \"40060\", \"40069\"\n",
        "]\n",
        "\n",
        "estrus_4 = {\n",
        "    '40054': ('2022-11-09', \"6\"),\n",
        "    '40086': ('2022-11-10', \"7\"),\n",
        "    '40051': ('2022-11-08', \"5\"),\n",
        "    '40040': ('2022-11-09', \"6\"),\n",
        "    '40319': ('2022-11-08', \"5\"),\n",
        "    '40115': ('2022-11-09', \"6\"),\n",
        "    #'40987': ('2022-11-11', \"no\"),\n",
        "    '40141': ('2022-11-08', \"5\"),\n",
        "    '40883': ('2022-11-09', \"6\"),\n",
        "    '40026': ('2022-11-08', \"5\"),\n",
        "    '40867': ('2022-11-08', \"5\"),\n",
        "    '40060': ('2022-11-08', \"5\"),\n",
        "    '40069': ('2022-11-08', \"5\"),\n",
        "}\n",
        "\n",
        "\n",
        "# Batch 5, total 14,\n",
        "pig_5 = [\n",
        "    \"50805\", \"50089\", \"50221\", \"50151\", \"50974\", \"50919\", \"50094\",\n",
        "    \"50222\", \"50981\", \"50043\", \"50078\", \"50079\", \"50031\", \"50152\"\n",
        "]\n",
        "estrus_5 = {\n",
        " '50805': ('2022-12-15', \"7\"),\n",
        "    '50089': ('2022-12-14', \"6\"),\n",
        "    '50221': ('2022-12-13', \"5\"),\n",
        "    '50151': ('2022-12-14', \"6\"),\n",
        "    '50974': ('2022-12-13', \"5\"),\n",
        "    '50919': ('2022-12-13', \"5\"),\n",
        "    '50094': ('2022-12-14', \"6\"),\n",
        "    '50222': ('2022-12-14', \"6\"),\n",
        "    '50981': ('2022-12-14', \"6\"),\n",
        "    '50043': ('2022-12-13', \"5\"),\n",
        "    '50078': ('2022-12-15', \"7\"),\n",
        "    '50079': ('2022-12-13', \"6\"),\n",
        "    '50031': ('2022-12-14', \"6\"),\n",
        "    '50152': ('2022-12-14', \"6\"),\n",
        "}\n",
        "\n",
        "\n",
        "# Batch 6, total 11,  estrus data missing 60050, 60216,60091,60098 60506 60927 60777\n",
        "pig_6 = [\n",
        "    \"60809\", \"60206\", \"60048\", \"60984\", \"60092\",\n",
        "    \"60038\", \"60039\", \"60155\", \"60093\", \"60866\", \"60049\"\n",
        "]\n",
        "\n",
        "estrus_6= {\n",
        "    '60809': ('2023-01-17', \"5\"),\n",
        "    '60206': ('2023-01-17', \"5\"),\n",
        "    '60048': ('2023-01-18', \"6\"),\n",
        "    '60984': ('2023-01-17', \"5\"),\n",
        "    '60092': ('2023-01-17', \"5\"),\n",
        "    '60038': ('2023-01-18', \"6\"),\n",
        "    '60039': ('2023-01-18', \"6\"),\n",
        "    '60155': ('2023-01-17', \"5\"),\n",
        "    '60093': ('2023-01-18', \"6\"),\n",
        "    '60866': ('2023-01-17', \"5\"),\n",
        "    '60049': ('2023-01-19', \"7\")\n",
        "    }\n",
        "\n",
        "# Batch 7, total 8, esturs data missing 70204, 70096, 70049, 70951, 70111\n",
        "\n",
        "pig_7 = [\n",
        "    \"70157\", \"70947\", \"70040\", \"70950\",\n",
        "    \"70014\", \"70814\",  \"70790\", \"70951\"\n",
        "]\n",
        "\n",
        "estrus_7 = {\n",
        "    '70157': ('2023-02-22', \"6\"),\n",
        "    '70947': ('2023-02-23', \"7\"),\n",
        "    '70040': ('2023-02-22', \"6\"),\n",
        "    '70950': ('2023-02-22', \"6\"),\n",
        "    '70014': ('2023-02-21', \"5\"),\n",
        "    '70814': ('2023-02-21', \"5\"),\n",
        "    '70790': ('2023-02-21', \"5\"),\n",
        "    '70951': ('2023-02-21', \"5\")\n",
        "}\n",
        "\n",
        "\n",
        "all_pigs = pig_1 + pig_2 + pig_3 + pig_4 + pig_5 + pig_6 + pig_7\n",
        "estrus_dates = { **estrus_1, **estrus_2, **estrus_3, **estrus_4, **estrus_5, **estrus_6, **estrus_7}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTzFv2bWqiIa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0MSVn2U4Bs22"
      },
      "outputs": [],
      "source": [
        "def split_pigs(all_pigs, estrus_dates, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Split pigs into training, validation, and test sets while maintaining class distribution\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Group pigs by their estrus day\n",
        "    day_groups = {}\n",
        "    for pig in all_pigs:\n",
        "        day = estrus_dates[pig][1]\n",
        "        if day not in day_groups:\n",
        "            day_groups[day] = []\n",
        "        day_groups[day].append(pig)\n",
        "\n",
        "    train_pigs = []\n",
        "    val_pigs = []\n",
        "    test_pigs = []\n",
        "\n",
        "    # Print initial group sizes\n",
        "    print(\"\\nInitial group sizes:\")\n",
        "    for day, pigs in day_groups.items():\n",
        "        print(f\"Day {day}: {len(pigs)} pigs\")\n",
        "\n",
        "    for day, pigs in day_groups.items():\n",
        "        np.random.shuffle(pigs)\n",
        "        n_pigs = len(pigs)\n",
        "\n",
        "        # Calculate exact numbers, ensuring they sum to n_pigs\n",
        "        n_train = round(n_pigs * train_ratio)\n",
        "        n_val = round(n_pigs * val_ratio)\n",
        "        n_test = n_pigs - n_train - n_val  # Ensure total adds up to n_pigs\n",
        "\n",
        "        print(f\"\\nSplitting Day {day} ({n_pigs} pigs):\")\n",
        "        print(f\"Training: {n_train} pigs ({n_train/n_pigs*100:.1f}%)\")\n",
        "        print(f\"Validation: {n_val} pigs ({n_val/n_pigs*100:.1f}%)\")\n",
        "        print(f\"Testing: {n_test} pigs ({n_test/n_pigs*100:.1f}%)\")\n",
        "\n",
        "        train_pigs.extend(pigs[:n_train])\n",
        "        val_pigs.extend(pigs[n_train:n_train + n_val])\n",
        "        test_pigs.extend(pigs[n_train + n_val:])\n",
        "\n",
        "    total_pigs = len(train_pigs) + len(val_pigs) + len(test_pigs)\n",
        "    print(f\"\\nFinal distribution:\")\n",
        "    print(f\"Training: {len(train_pigs)}/{total_pigs} ({len(train_pigs)/total_pigs*100:.1f}%)\")\n",
        "    print(f\"Validation: {len(val_pigs)}/{total_pigs} ({len(val_pigs)/total_pigs*100:.1f}%)\")\n",
        "    print(f\"Testing: {len(test_pigs)}/{total_pigs} ({len(test_pigs)/total_pigs*100:.1f}%)\")\n",
        "\n",
        "    return train_pigs, val_pigs, test_pigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDMV9tYTBs_h"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"\n",
        "    Set seeds for reproducibility\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99rL3m8tytDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJDQePCWzOx3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZrgfMie8yjs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TgM3JgTLT76a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedPigTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=5, num_heads=4, num_layers=4, d_model=128, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.input_norm = nn.LayerNorm(input_dim)\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, 144, d_model) * 0.02)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model*4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.attention_norm = nn.LayerNorm(d_model)\n",
        "        self.global_attention = nn.MultiheadAttention(\n",
        "            d_model,\n",
        "            1,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, d_model // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(d_model // 4, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.float()\n",
        "        x = self.input_norm(x)\n",
        "        x = self.input_projection(x)\n",
        "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "\n",
        "        x = self.attention_norm(x)\n",
        "        query = torch.mean(x, dim=1, keepdim=True)\n",
        "        pooled, _ = self.global_attention(query, x, x)\n",
        "        pooled = pooled.squeeze(1)\n",
        "\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "class ImprovedPigDataset(Dataset):\n",
        "    def __init__(self, pig_ids, sorted_files_dir, estrus_dates, mode='train', seq_length=144):\n",
        "        super().__init__()\n",
        "        self.pig_ids = pig_ids\n",
        "        self.sorted_files_dir = sorted_files_dir\n",
        "        self.estrus_dates = estrus_dates\n",
        "        self.seq_length = seq_length\n",
        "        self.mode = mode\n",
        "        self.data = self._load_all_data()\n",
        "        self.sequences = self._prepare_sequences()\n",
        "\n",
        "    def _get_label(self, pig_id):\n",
        "        if pig_id not in self.estrus_dates:\n",
        "            return 0\n",
        "        _, day = self.estrus_dates[pig_id]\n",
        "        if day is None or day == \"no\":\n",
        "            return 0\n",
        "        return int(day) - 4\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        all_data = {}\n",
        "        scaler = MinMaxScaler()\n",
        "\n",
        "        for pig_id in self.pig_ids:\n",
        "            try:\n",
        "                df = pd.read_csv(f\"{self.sorted_files_dir}/{pig_id}_segments_sorted.txt\")\n",
        "\n",
        "                df['timestamp'] = pd.to_datetime(df['filename'].apply(\n",
        "                    lambda x: '_'.join(x.split('_')[1:7])),\n",
        "                    format='%Y_%m_%d_%H_%M_%S')\n",
        "\n",
        "                estrus_date = pd.to_datetime(self.estrus_dates[pig_id][0]) if pig_id in self.estrus_dates else None\n",
        "\n",
        "                if estrus_date is not None:\n",
        "                    # Use 4 days before estrus date\n",
        "                    start_date = estrus_date - pd.Timedelta(days=4)\n",
        "                    end_date = estrus_date - pd.Timedelta(days=1)\n",
        "\n",
        "                    mask = df['timestamp'].between(start_date, end_date + pd.Timedelta(days=1) - pd.Timedelta(seconds=1))\n",
        "                    df = df[mask].copy()\n",
        "\n",
        "                    if len(df) == 0:\n",
        "                        print(f\"No data for pig {pig_id} in {self.mode} mode\")\n",
        "                        continue\n",
        "\n",
        "                    df['day_label'] = self._get_label(pig_id)\n",
        "\n",
        "                    # Feature processing\n",
        "                    df['normalized_area'] = scaler.fit_transform(\n",
        "                        df['segment_1_area_size'].values.reshape(-1, 1))\n",
        "\n",
        "                    df['hour'] = df['timestamp'].dt.hour\n",
        "                    df['minute'] = df['timestamp'].dt.minute\n",
        "                    df['time_of_day'] = df['hour'] + df['minute']/60\n",
        "\n",
        "                    df['hour_sin'] = np.sin(2 * np.pi * df['time_of_day'] / 24)\n",
        "                    df['hour_cos'] = np.cos(2 * np.pi * df['time_of_day'] / 24)\n",
        "\n",
        "                    df['area_change'] = df['normalized_area'].diff().fillna(0)\n",
        "                    df['activity_level'] = df['area_change'].abs()\n",
        "\n",
        "                    all_data[pig_id] = df\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing pig {pig_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _prepare_sequences(self):\n",
        "        sequences = []\n",
        "\n",
        "        for pig_id, df in self.data.items():\n",
        "            df = df.sort_values('timestamp')\n",
        "\n",
        "            for i in range(len(df) - self.seq_length + 1):\n",
        "                seq_df = df.iloc[i:i+self.seq_length]\n",
        "\n",
        "                seq_features = np.column_stack([\n",
        "                    seq_df['normalized_area'].values,\n",
        "                    seq_df['area_change'].values,\n",
        "                    seq_df['activity_level'].values,\n",
        "                    seq_df['hour_sin'].values,\n",
        "                    seq_df['hour_cos'].values\n",
        "                ])\n",
        "\n",
        "                seq_day_label = seq_df['day_label'].iloc[0]\n",
        "\n",
        "                sequences.append({\n",
        "                    'pig_id': pig_id,\n",
        "                    'features': seq_features,\n",
        "                    'day_label': seq_day_label,\n",
        "                    'start_time': seq_df['timestamp'].iloc[0]\n",
        "                })\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        return {\n",
        "            'pig_id': seq['pig_id'],\n",
        "            'features': torch.FloatTensor(seq['features']),\n",
        "            'day_label': torch.LongTensor([seq['day_label']])[0],\n",
        "            'start_time': seq['start_time'].strftime('%Y_%m_%d_%H_%M_%S')\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puXtswMmeYVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jq2WNojheZ7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_validate_test(train_pigs, val_pigs, test_pigs, estrus_dates, base_dir=\".\", seed=42):\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    set_seed(seed)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ImprovedPigDataset(\n",
        "        pig_ids=train_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='train'\n",
        "    )\n",
        "\n",
        "    val_dataset = ImprovedPigDataset(\n",
        "        pig_ids=val_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='val'\n",
        "    )\n",
        "\n",
        "    test_dataset = ImprovedPigDataset(\n",
        "        pig_ids=test_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='test'\n",
        "    )\n",
        "\n",
        "    # Create data loaders with worker seed\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedPigTransformer(\n",
        "        input_dim=5,\n",
        "        num_heads=2,\n",
        "        num_layers=2,\n",
        "        d_model=128,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate class weights for balanced training\n",
        "    labels = [data['day_label'] for data in train_dataset]\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = torch.FloatTensor(1.0 / np.maximum(class_counts, 1)).to(device)\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "    num_epochs = 50\n",
        "    best_val_f1 = 0\n",
        "    patience = 10\n",
        "    no_improve = 0\n",
        "\n",
        "    # Print initial setup\n",
        "    print(f\"\\nTraining with seed: {seed}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "    print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            features = batch['features'].float().to(device)\n",
        "            labels = batch['day_label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                features = batch['features'].float().to(device)\n",
        "                labels = batch['day_label'].to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, F1: {train_f1:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_f1': train_f1,\n",
        "                'val_f1': val_f1,\n",
        "                'seed': seed,\n",
        "                'class_weights': class_weights\n",
        "            }, 'best_model.pth')\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    # Load best model for testing\n",
        "    checkpoint = torch.load('best_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Test phase\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            features = batch['features'].float().to(device)\n",
        "            labels = batch['day_label'].to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            test_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate and print final metrics\n",
        "    print(\"\\nFinal Results:\")\n",
        "\n",
        "    print(\"\\nTRAINING SET:\")\n",
        "    train_metrics = classification_report(train_labels, train_preds)\n",
        "    print(train_metrics)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(train_labels, train_preds))\n",
        "\n",
        "    print(\"\\nVALIDATION SET:\")\n",
        "    val_metrics = classification_report(val_labels, val_preds)\n",
        "    print(val_metrics)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(val_labels, val_preds))\n",
        "\n",
        "    print(\"\\nTEST SET:\")\n",
        "    test_metrics = classification_report(test_labels, test_preds)\n",
        "    print(test_metrics)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels, test_preds))\n",
        "\n",
        "    return model, {\n",
        "        'train_metrics': classification_report(train_labels, train_preds, output_dict=True),\n",
        "        'val_metrics': classification_report(val_labels, val_preds, output_dict=True),\n",
        "        'test_metrics': classification_report(test_labels, test_preds, output_dict=True),\n",
        "        'train_conf': confusion_matrix(train_labels, train_preds),\n",
        "        'val_conf': confusion_matrix(val_labels, val_preds),\n",
        "        'test_conf': confusion_matrix(test_labels, test_preds),\n",
        "        'seed': seed,\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'final_train_f1': train_f1,\n",
        "        'final_val_f1': val_f1,\n",
        "        'best_val_f1': checkpoint['val_f1']\n",
        "    }\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set global seed\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # Get train/val/test splits\n",
        "    train_pigs, val_pigs, test_pigs = split_pigs(all_pigs, estrus_dates, seed=SEED)\n",
        "\n",
        "    print(f\"Running with seed: {SEED}\")\n",
        "    model, results = train_validate_test(\n",
        "        train_pigs=train_pigs,\n",
        "        val_pigs=val_pigs,\n",
        "        test_pigs=test_pigs,\n",
        "        estrus_dates=estrus_dates,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    # Save complete results\n",
        "    torch.save({\n",
        "        'results': results,\n",
        "        'seed': SEED,\n",
        "        'train_pigs': train_pigs,\n",
        "        'val_pigs': val_pigs,\n",
        "        'test_pigs': test_pigs\n",
        "    }, f'complete_results_seed_{SEED}.pth')"
      ],
      "metadata": {
        "id": "E2SY1cnAeadZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8bec9d-c1c6-4c01-a1d3-7e5684a9b06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial group sizes:\n",
            "Day 6: 28 pigs\n",
            "Day 5: 33 pigs\n",
            "Day 7: 10 pigs\n",
            "\n",
            "Splitting Day 6 (28 pigs):\n",
            "Training: 17 pigs (60.7%)\n",
            "Validation: 6 pigs (21.4%)\n",
            "Testing: 5 pigs (17.9%)\n",
            "\n",
            "Splitting Day 5 (33 pigs):\n",
            "Training: 20 pigs (60.6%)\n",
            "Validation: 7 pigs (21.2%)\n",
            "Testing: 6 pigs (18.2%)\n",
            "\n",
            "Splitting Day 7 (10 pigs):\n",
            "Training: 6 pigs (60.0%)\n",
            "Validation: 2 pigs (20.0%)\n",
            "Testing: 2 pigs (20.0%)\n",
            "\n",
            "Final distribution:\n",
            "Training: 43/71 (60.6%)\n",
            "Validation: 15/71 (21.1%)\n",
            "Testing: 13/71 (18.3%)\n",
            "Running with seed: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with seed: 42\n",
            "Device: cuda\n",
            "Training samples: 14353\n",
            "Validation samples: 4885\n",
            "Test samples: 4362\n",
            "Class weights: [9.9922550e-01 1.6762715e-04 1.6266083e-04 4.4429768e-04]\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/50\n",
            "Train Loss: 1.0241, F1: 0.4832\n",
            "Val Loss: 1.1276, F1: 0.3939\n",
            "\n",
            "Epoch 2/50\n",
            "Train Loss: 0.6903, F1: 0.7123\n",
            "Val Loss: 1.3554, F1: 0.5224\n",
            "\n",
            "Epoch 3/50\n",
            "Train Loss: 0.4440, F1: 0.8283\n",
            "Val Loss: 1.9585, F1: 0.4642\n",
            "\n",
            "Epoch 4/50\n",
            "Train Loss: 0.3146, F1: 0.9033\n",
            "Val Loss: 2.4474, F1: 0.5238\n",
            "\n",
            "Epoch 5/50\n",
            "Train Loss: 0.2324, F1: 0.9423\n",
            "Val Loss: 2.6223, F1: 0.5338\n",
            "\n",
            "Epoch 6/50\n",
            "Train Loss: 0.1881, F1: 0.9581\n",
            "Val Loss: 3.2124, F1: 0.4945\n",
            "\n",
            "Epoch 7/50\n",
            "Train Loss: 0.1482, F1: 0.9690\n",
            "Val Loss: 3.2855, F1: 0.5066\n",
            "\n",
            "Epoch 8/50\n",
            "Train Loss: 0.1347, F1: 0.9712\n",
            "Val Loss: 4.2506, F1: 0.4596\n",
            "\n",
            "Epoch 9/50\n",
            "Train Loss: 0.1313, F1: 0.9740\n",
            "Val Loss: 4.1798, F1: 0.4423\n",
            "\n",
            "Epoch 10/50\n",
            "Train Loss: 0.1048, F1: 0.9800\n",
            "Val Loss: 3.5501, F1: 0.4571\n",
            "\n",
            "Epoch 11/50\n",
            "Train Loss: 0.1060, F1: 0.9788\n",
            "Val Loss: 3.9286, F1: 0.4418\n",
            "\n",
            "Epoch 12/50\n",
            "Train Loss: 0.0856, F1: 0.9838\n",
            "Val Loss: 4.6106, F1: 0.4439\n",
            "\n",
            "Epoch 13/50\n",
            "Train Loss: 0.0735, F1: 0.9859\n",
            "Val Loss: 3.8218, F1: 0.5143\n",
            "\n",
            "Epoch 14/50\n",
            "Train Loss: 0.0734, F1: 0.9867\n",
            "Val Loss: 4.4534, F1: 0.4873\n",
            "\n",
            "Epoch 15/50\n",
            "Train Loss: 0.0793, F1: 0.9861\n",
            "Val Loss: 4.4049, F1: 0.4619\n",
            "\n",
            "Epoch 16/50\n",
            "Train Loss: 0.0598, F1: 0.9885\n",
            "Val Loss: 4.0546, F1: 0.5154\n",
            "\n",
            "Epoch 17/50\n",
            "Train Loss: 0.0553, F1: 0.9904\n",
            "Val Loss: 4.4566, F1: 0.4792\n",
            "\n",
            "Epoch 18/50\n",
            "Train Loss: 0.0577, F1: 0.9884\n",
            "Val Loss: 3.5415, F1: 0.5493\n",
            "\n",
            "Epoch 19/50\n",
            "Train Loss: 0.0538, F1: 0.9902\n",
            "Val Loss: 4.9997, F1: 0.4426\n",
            "\n",
            "Epoch 20/50\n",
            "Train Loss: 0.0419, F1: 0.9919\n",
            "Val Loss: 3.9448, F1: 0.5395\n",
            "\n",
            "Epoch 21/50\n",
            "Train Loss: 0.0516, F1: 0.9908\n",
            "Val Loss: 3.4837, F1: 0.5531\n",
            "\n",
            "Epoch 22/50\n",
            "Train Loss: 0.0475, F1: 0.9919\n",
            "Val Loss: 4.8374, F1: 0.3888\n",
            "\n",
            "Epoch 23/50\n",
            "Train Loss: 0.0396, F1: 0.9927\n",
            "Val Loss: 4.7339, F1: 0.4822\n",
            "\n",
            "Epoch 24/50\n",
            "Train Loss: 0.0388, F1: 0.9940\n",
            "Val Loss: 4.1558, F1: 0.5054\n",
            "\n",
            "Epoch 25/50\n",
            "Train Loss: 0.0425, F1: 0.9931\n",
            "Val Loss: 4.1465, F1: 0.5283\n",
            "\n",
            "Epoch 26/50\n",
            "Train Loss: 0.0321, F1: 0.9937\n",
            "Val Loss: 4.9735, F1: 0.4729\n",
            "\n",
            "Epoch 27/50\n",
            "Train Loss: 0.0330, F1: 0.9943\n",
            "Val Loss: 4.7900, F1: 0.4874\n",
            "\n",
            "Epoch 28/50\n",
            "Train Loss: 0.0357, F1: 0.9937\n",
            "Val Loss: 5.4717, F1: 0.4684\n",
            "\n",
            "Epoch 29/50\n",
            "Train Loss: 0.0368, F1: 0.9943\n",
            "Val Loss: 4.4528, F1: 0.5038\n",
            "\n",
            "Epoch 30/50\n",
            "Train Loss: 0.0352, F1: 0.9942\n",
            "Val Loss: 4.9275, F1: 0.4584\n",
            "\n",
            "Epoch 31/50\n",
            "Train Loss: 0.0358, F1: 0.9947\n",
            "Val Loss: 5.1527, F1: 0.4882\n",
            "\n",
            "Epoch 32/50\n",
            "Train Loss: 0.0236, F1: 0.9957\n",
            "Val Loss: 5.3084, F1: 0.4791\n",
            "\n",
            "Epoch 33/50\n",
            "Train Loss: 0.0234, F1: 0.9958\n",
            "Val Loss: 5.1431, F1: 0.4861\n",
            "\n",
            "Epoch 34/50\n",
            "Train Loss: 0.0277, F1: 0.9948\n",
            "Val Loss: 3.9257, F1: 0.5568\n",
            "\n",
            "Epoch 35/50\n",
            "Train Loss: 0.0319, F1: 0.9944\n",
            "Val Loss: 3.7344, F1: 0.5635\n",
            "\n",
            "Epoch 36/50\n",
            "Train Loss: 0.0234, F1: 0.9959\n",
            "Val Loss: 3.3784, F1: 0.5825\n",
            "\n",
            "Epoch 37/50\n",
            "Train Loss: 0.0280, F1: 0.9951\n",
            "Val Loss: 4.0565, F1: 0.5335\n",
            "\n",
            "Epoch 38/50\n",
            "Train Loss: 0.0242, F1: 0.9949\n",
            "Val Loss: 4.6109, F1: 0.5161\n",
            "\n",
            "Epoch 39/50\n",
            "Train Loss: 0.0196, F1: 0.9964\n",
            "Val Loss: 4.3656, F1: 0.5405\n",
            "\n",
            "Epoch 40/50\n",
            "Train Loss: 0.0285, F1: 0.9951\n",
            "Val Loss: 5.0395, F1: 0.4776\n",
            "\n",
            "Epoch 41/50\n",
            "Train Loss: 0.0268, F1: 0.9958\n",
            "Val Loss: 4.3362, F1: 0.5234\n",
            "\n",
            "Epoch 42/50\n",
            "Train Loss: 0.0266, F1: 0.9951\n",
            "Val Loss: 4.2657, F1: 0.5309\n",
            "\n",
            "Epoch 43/50\n",
            "Train Loss: 0.0196, F1: 0.9969\n",
            "Val Loss: 4.6599, F1: 0.5110\n",
            "\n",
            "Epoch 44/50\n",
            "Train Loss: 0.0218, F1: 0.9967\n",
            "Val Loss: 4.7443, F1: 0.5266\n",
            "\n",
            "Epoch 45/50\n",
            "Train Loss: 0.0267, F1: 0.9950\n",
            "Val Loss: 5.2188, F1: 0.4579\n",
            "\n",
            "Epoch 46/50\n",
            "Train Loss: 0.0174, F1: 0.9968\n",
            "Val Loss: 4.5319, F1: 0.5456\n",
            "\n",
            "Epoch 47/50\n",
            "Train Loss: 0.0250, F1: 0.9956\n",
            "Val Loss: 4.5767, F1: 0.4974\n",
            "\n",
            "Epoch 48/50\n",
            "Train Loss: 0.0195, F1: 0.9969\n",
            "Val Loss: 5.1868, F1: 0.5010\n",
            "\n",
            "Epoch 49/50\n",
            "Train Loss: 0.0195, F1: 0.9964\n",
            "Val Loss: 5.3621, F1: 0.4419\n",
            "\n",
            "Epoch 50/50\n",
            "Train Loss: 0.0224, F1: 0.9962\n",
            "Val Loss: 4.2155, F1: 0.5035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b21d1b1d97ff>:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('best_model.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results:\n",
            "\n",
            "TRAINING SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      5961\n",
            "           2       1.00      1.00      1.00      6143\n",
            "           3       1.00      0.99      0.99      2249\n",
            "\n",
            "    accuracy                           1.00     14353\n",
            "   macro avg       1.00      1.00      1.00     14353\n",
            "weighted avg       1.00      1.00      1.00     14353\n",
            "\n",
            "Confusion Matrix:\n",
            "[[5938   17    6]\n",
            " [  14 6125    4]\n",
            " [   5    9 2235]]\n",
            "\n",
            "VALIDATION SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.60      0.53      0.56      1990\n",
            "           2       0.53      0.73      0.62      2174\n",
            "           3       0.03      0.01      0.01       721\n",
            "\n",
            "    accuracy                           0.54      4885\n",
            "   macro avg       0.39      0.42      0.39      4885\n",
            "weighted avg       0.48      0.54      0.50      4885\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1045  916   29]\n",
            " [ 471 1592  111]\n",
            " [ 222  495    4]]\n",
            "\n",
            "TEST SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.30      0.34      0.32      1738\n",
            "           2       0.33      0.39      0.36      1818\n",
            "           3       0.09      0.02      0.04       806\n",
            "\n",
            "    accuracy                           0.30      4362\n",
            "   macro avg       0.24      0.25      0.24      4362\n",
            "weighted avg       0.27      0.30      0.28      4362\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 591 1134   13]\n",
            " [ 936  710  172]\n",
            " [ 464  324   18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqQfyPm_edB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxW2BWAvRkhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WU6jjeuTRkym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 622"
      ],
      "metadata": {
        "id": "ImzaQA10SZSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import itertools\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"\n",
        "    Set seeds for reproducibility with enhanced deterministic behavior\n",
        "    \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def create_deterministic_loader(dataset, batch_size, shuffle, num_workers, seed):\n",
        "    \"\"\"\n",
        "    Create a deterministic DataLoader\n",
        "    \"\"\"\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(seed)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id),\n",
        "        generator=generator,\n",
        "        persistent_workers=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "def optimize_and_train(train_pigs, val_pigs, test_pigs, estrus_dates, base_dir=\".\", seed=42):\n",
        "    \"\"\"\n",
        "    Complete pipeline with optimization, training, validation, and testing\n",
        "    \"\"\"\n",
        "    # Set seeds for reproducibility\n",
        "    set_seed(seed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ImprovedPigDataset(\n",
        "        pig_ids=train_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='train'\n",
        "    )\n",
        "\n",
        "    val_dataset = ImprovedPigDataset(\n",
        "        pig_ids=val_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='val'\n",
        "    )\n",
        "\n",
        "    test_dataset = ImprovedPigDataset(\n",
        "        pig_ids=test_pigs,\n",
        "        sorted_files_dir=base_dir,\n",
        "        estrus_dates=estrus_dates,\n",
        "        mode='test'\n",
        "    )\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    param_grid = {\n",
        "        'num_heads': [4,8],\n",
        "        'num_layers': [4, 6],\n",
        "        'd_model': [128, 256],\n",
        "        'dropout': [0.1],\n",
        "        'learning_rate': [1e-4, 3e-4, 1e-3],\n",
        "        'batch_size': [16, 32]\n",
        "    }\n",
        "\n",
        "    # Optimization phase\n",
        "    print(\"Starting hyperparameter optimization...\")\n",
        "    best_params = optimize_hyperparameters(\n",
        "        train_dataset, val_dataset, param_grid, device, seed=seed\n",
        "    )\n",
        "    print(\"\\nBest hyperparameters found:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    # Create final data loaders with best batch size\n",
        "    train_loader = create_deterministic_loader(\n",
        "        train_dataset,\n",
        "        batch_size=best_params['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    val_loader = create_deterministic_loader(\n",
        "        val_dataset,\n",
        "        batch_size=best_params['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    test_loader = create_deterministic_loader(\n",
        "        test_dataset,\n",
        "        batch_size=best_params['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    final_model, results = train_with_params(\n",
        "        train_loader, val_loader, test_loader, best_params, device, seed=seed\n",
        "    )\n",
        "\n",
        "    return final_model, results, best_params\n",
        "\n",
        "def optimize_hyperparameters(train_data, val_data, param_grid, device, seed=42):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter optimization using cross-validation\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    best_score = -1\n",
        "    best_params = None\n",
        "\n",
        "    # Get class weights for balanced training\n",
        "    labels = [data['day_label'] for data in train_data]\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = torch.FloatTensor(1.0 / np.maximum(class_counts, 1)).to(device)\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "    # Generate parameter combinations\n",
        "    param_combinations = [dict(zip(param_grid.keys(), v))\n",
        "                        for v in itertools.product(*param_grid.values())]\n",
        "\n",
        "    for params in param_combinations:\n",
        "        print(f\"\\nTesting parameters: {params}\")\n",
        "        set_seed(seed)  # Reset seed for each parameter combination\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = create_deterministic_loader(\n",
        "            train_data,\n",
        "            batch_size=params['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        val_loader = create_deterministic_loader(\n",
        "            val_data,\n",
        "            batch_size=params['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = ImprovedPigTransformer(\n",
        "            input_dim=5,\n",
        "            num_heads=params['num_heads'],\n",
        "            num_layers=params['num_layers'],\n",
        "            d_model=params['d_model'],\n",
        "            dropout=params['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "        # Quick training to evaluate parameters\n",
        "        val_f1 = train_evaluation_run(\n",
        "            model, train_loader, val_loader, criterion, optimizer, device, seed=seed\n",
        "        )\n",
        "\n",
        "        print(f\"Validation F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_score:\n",
        "            best_score = val_f1\n",
        "            best_params = params.copy()  # Make a copy to ensure independence\n",
        "            print(\"New best parameters found!\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "\n",
        "def train_evaluation_run(model, train_loader, val_loader, criterion, optimizer, device,\n",
        "                        max_epochs=10, seed=42):\n",
        "    \"\"\"\n",
        "    Quick training run to evaluate parameters\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            features = batch['features'].float().to(device)\n",
        "            labels = batch['day_label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                features = batch['features'].float().to(device)\n",
        "                labels = batch['day_label'].to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        best_val_f1 = max(best_val_f1, val_f1)\n",
        "\n",
        "    return best_val_f1\n",
        "\n",
        "\n",
        "def train_with_params(train_loader, val_loader, test_loader, params, device, seed=42):\n",
        "    \"\"\"\n",
        "    Train final model with best parameters\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Initialize model with best parameters\n",
        "    model = ImprovedPigTransformer(\n",
        "        input_dim=5,\n",
        "        num_heads=params['num_heads'],\n",
        "        num_layers=params['num_layers'],\n",
        "        d_model=params['d_model'],\n",
        "        dropout=params['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    # Get class weights\n",
        "    labels = []\n",
        "    for batch in train_loader:\n",
        "        labels.extend(batch['day_label'].numpy())\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = torch.FloatTensor(1.0 / np.maximum(class_counts, 1)).to(device)\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=params['learning_rate'],\n",
        "        epochs=50,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0\n",
        "    patience = 10\n",
        "    no_improve = 0\n",
        "\n",
        "    print(\"\\nTraining final model...\")\n",
        "    for epoch in range(50):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            features = batch['features'].float().to(device)\n",
        "            labels = batch['day_label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                features = batch['features'].float().to(device)\n",
        "                labels = batch['day_label'].to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/50\")\n",
        "        print(f\"Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_val_f1': best_val_f1,\n",
        "                'seed': seed,\n",
        "                'params': params\n",
        "            }, 'best_model.pth')\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    # Load best model for testing\n",
        "    checkpoint = torch.load('best_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Final evaluation on all sets\n",
        "    results = evaluate_model(model, train_loader, val_loader, test_loader, device)\n",
        "\n",
        "    # Add training metadata to results\n",
        "    results['training_metadata'] = {\n",
        "        'seed': seed,\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'best_val_f1': checkpoint['best_val_f1'],\n",
        "        'params': params\n",
        "    }\n",
        "\n",
        "    return model, results\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on all datasets\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    # Evaluate each dataset\n",
        "    for name, loader in [('train', train_loader), ('val', val_loader), ('test', test_loader)]:\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                features = batch['features'].float().to(device)\n",
        "                labels = batch['day_label'].to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                predictions.extend(outputs.argmax(1).cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = classification_report(true_labels, predictions, output_dict=True)\n",
        "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "        results[name] = {\n",
        "            'metrics': metrics,\n",
        "            'confusion_matrix': conf_matrix\n",
        "        }\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n{name.upper()} SET RESULTS:\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(true_labels, predictions))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(conf_matrix)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set environment variables for deterministic behavior\n",
        "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "    # Set global seed\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # Verify seed is working\n",
        "    print(\"Verifying seed reproducibility:\")\n",
        "    print(f\"Random number 1: {random.random()}\")\n",
        "    print(f\"Numpy random 1: {np.random.rand(1)[0]}\")\n",
        "    print(f\"Torch random 1: {torch.rand(1)[0].item()}\")\n",
        "\n",
        "    # Split pigs\n",
        "    train_pigs, val_pigs, test_pigs = split_pigs(all_pigs, estrus_dates, seed=SEED)\n",
        "\n",
        "    print(f\"\\nNumber of pigs in training set: {len(train_pigs)}\")\n",
        "    print(f\"Number of pigs in validation set: {len(val_pigs)}\")\n",
        "    print(f\"Number of pigs in test set: {len(test_pigs)}\")\n",
        "\n",
        "    print(f\"\\nRunning with seed: {SEED}\")\n",
        "    model, results, best_params = optimize_and_train(\n",
        "        train_pigs=train_pigs,\n",
        "        val_pigs=val_pigs,\n",
        "        test_pigs=test_pigs,\n",
        "        estrus_dates=estrus_dates,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    # Save complete results with verification info\n",
        "    torch.save({\n",
        "        'results': results,\n",
        "        'best_params': best_params,\n",
        "        'seed': SEED,\n",
        "        'train_pigs': train_pigs,\n",
        "        'val_pigs': val_pigs,\n",
        "        'test_pigs': test_pigs,\n",
        "        'random_verification': {\n",
        "            'random_num': random.random(),\n",
        "            'numpy_random': np.random.rand(1)[0],\n",
        "            'torch_random': torch.rand(1)[0].item()\n",
        "        },\n",
        "        'environment_info': {\n",
        "            'pytorch_version': torch.__version__,\n",
        "            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n",
        "            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "        }\n",
        "    }, f'complete_results_seed_{SEED}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9qMbzm_RlFe",
        "outputId": "6a034b31-0d15-489a-e861-02129f4ded41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying seed reproducibility:\n",
            "Random number 1: 0.6394267984578837\n",
            "Numpy random 1: 0.3745401188473625\n",
            "Torch random 1: 0.8822692632675171\n",
            "\n",
            "Initial group sizes:\n",
            "Day 6: 28 pigs\n",
            "Day 5: 33 pigs\n",
            "Day 7: 10 pigs\n",
            "\n",
            "Splitting Day 6 (28 pigs):\n",
            "Training: 17 pigs (60.7%)\n",
            "Validation: 6 pigs (21.4%)\n",
            "Testing: 5 pigs (17.9%)\n",
            "\n",
            "Splitting Day 5 (33 pigs):\n",
            "Training: 20 pigs (60.6%)\n",
            "Validation: 7 pigs (21.2%)\n",
            "Testing: 6 pigs (18.2%)\n",
            "\n",
            "Splitting Day 7 (10 pigs):\n",
            "Training: 6 pigs (60.0%)\n",
            "Validation: 2 pigs (20.0%)\n",
            "Testing: 2 pigs (20.0%)\n",
            "\n",
            "Final distribution:\n",
            "Training: 43/71 (60.6%)\n",
            "Validation: 15/71 (21.1%)\n",
            "Testing: 13/71 (18.3%)\n",
            "\n",
            "Number of pigs in training set: 43\n",
            "Number of pigs in validation set: 15\n",
            "Number of pigs in test set: 13\n",
            "\n",
            "Running with seed: 42\n",
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n",
            "CUDA Version: 12.1\n",
            "PyTorch Version: 2.5.1+cu121\n",
            "Starting hyperparameter optimization...\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5222\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5257\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5440\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4679\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4756\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5440\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5560\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4776\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4878\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5308\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5668\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5342\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4767\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2800\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.3255\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5611\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6334\n",
            "New best parameters found!\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4574\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4377\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 4, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5642\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6129\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5107\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4612\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5389\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5140\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4462\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5189\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 4, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5697\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5633\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5375\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5121\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 128, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5200\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5339\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.4464\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.0003, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.5916\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Testing parameters: {'num_heads': 8, 'num_layers': 6, 'd_model': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.2741\n",
            "\n",
            "Best hyperparameters found:\n",
            "num_heads: 4\n",
            "num_layers: 6\n",
            "d_model: 256\n",
            "dropout: 0.1\n",
            "learning_rate: 0.0001\n",
            "batch_size: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model...\n",
            "\n",
            "Epoch 1/50\n",
            "Train F1: 0.3847\n",
            "Val F1: 0.2501\n",
            "\n",
            "Epoch 2/50\n",
            "Train F1: 0.4229\n",
            "Val F1: 0.5073\n",
            "\n",
            "Epoch 3/50\n",
            "Train F1: 0.4605\n",
            "Val F1: 0.5164\n",
            "\n",
            "Epoch 4/50\n",
            "Train F1: 0.5106\n",
            "Val F1: 0.4265\n",
            "\n",
            "Epoch 5/50\n",
            "Train F1: 0.5694\n",
            "Val F1: 0.4160\n",
            "\n",
            "Epoch 6/50\n",
            "Train F1: 0.6602\n",
            "Val F1: 0.4597\n",
            "\n",
            "Epoch 7/50\n",
            "Train F1: 0.7633\n",
            "Val F1: 0.4351\n",
            "\n",
            "Epoch 8/50\n",
            "Train F1: 0.8675\n",
            "Val F1: 0.4741\n",
            "\n",
            "Epoch 9/50\n",
            "Train F1: 0.9214\n",
            "Val F1: 0.4178\n",
            "\n",
            "Epoch 10/50\n",
            "Train F1: 0.9416\n",
            "Val F1: 0.4848\n",
            "\n",
            "Epoch 11/50\n",
            "Train F1: 0.9592\n",
            "Val F1: 0.4593\n",
            "\n",
            "Epoch 12/50\n",
            "Train F1: 0.9685\n",
            "Val F1: 0.4566\n",
            "\n",
            "Epoch 13/50\n",
            "Train F1: 0.9733\n",
            "Val F1: 0.4947\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b571cbca0e7f>:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('best_model.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAIN SET RESULTS:\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.47      0.85      0.60      5961\n",
            "           2       0.59      0.29      0.39      6143\n",
            "           3       0.78      0.12      0.21      2249\n",
            "\n",
            "    accuracy                           0.50     14353\n",
            "   macro avg       0.61      0.42      0.40     14353\n",
            "weighted avg       0.57      0.50      0.45     14353\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[5094  854   13]\n",
            " [4267 1810   66]\n",
            " [1586  385  278]]\n",
            "\n",
            "VAL SET RESULTS:\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.48      0.79      0.60      1990\n",
            "           2       0.72      0.53      0.61      2174\n",
            "           3       0.00      0.00      0.00       721\n",
            "\n",
            "    accuracy                           0.56      4885\n",
            "   macro avg       0.40      0.44      0.40      4885\n",
            "weighted avg       0.52      0.56      0.52      4885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1578  386   26]\n",
            " [1024 1150    0]\n",
            " [ 665   56    0]]\n",
            "\n",
            "TEST SET RESULTS:\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.25      0.29      0.27      1738\n",
            "           2       0.41      0.52      0.46      1818\n",
            "           3       0.00      0.00      0.00       806\n",
            "\n",
            "    accuracy                           0.33      4362\n",
            "   macro avg       0.22      0.27      0.24      4362\n",
            "weighted avg       0.27      0.33      0.30      4362\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 506 1232    0]\n",
            " [ 871  938    9]\n",
            " [ 678  128    0]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}